{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ba24238c-c13f-43d4-bb1a-abe8fb226c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from deep_translator import GoogleTranslator\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import gradio as gr\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e148c62b-affd-4102-9f4f-a93f8d326d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCRIPTION = '''\n",
    "<div>\n",
    "<h1 style=\"text-align: center;\">DTT Chatbot Piloto - Gemma 2B-it</h1>\n",
    "<p> Este espacio es un piloto sobre el uso de un \"instruction-tuned model\", en este caso <a href=\"https://huggingface.co/google/gemma-1.1-2b-it\"><b>Gemma 2B Chat</b></a>. Gemma es un LLM de c√≥digo abierto, el cual viene en dos tama√±os: 2B y 8B.</p>\n",
    "<p>üîé Para m√°s detalles sobre el modelo y sus capacidades de Gemma con <code>transformers</code>, puedes visitar <a href=\"https://huggingface.co/blog/gemma\">este blog</a>.</p>\n",
    "<p>ü¶ï Este chatbot piloto tiene como finalidad responder preguntas orientadas a los practicantes de la ECYS. Este chatbot fue alimentado con informaci√≥n oficial provista por el DTT.</p>\n",
    "<p>‚ùó¬øEl chatbot est√° tardando mucho en responder o no responde?‚ùó ¬°Presiona el bot√≥n de \"Aviso de p√°nico\" para enviar un aviso y que el chatbot sea reseteado manualmente!</p>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "LICENSE = \"\"\"\n",
    "<p/>\n",
    "---\n",
    "Built with Gemma 2B-it\n",
    "\"\"\"\n",
    "\n",
    "PLACEHOLDER = \"\"\"\n",
    "<div style=\"padding: 30px; text-align: center; display: flex; flex-direction: column; align-items: center;\">\n",
    "   <img src=\"https://www.lavanguardia.com/andro4all/hero/2024/04/google-gemma.png?width=1200&aspect_ratio=16:9\" style=\"width: 80%; max-width: 550px; height: auto; opacity: 0.55;\"> \n",
    "   <h1 style=\"font-size: 28px; margin-bottom: 2px; opacity: 0.55;\">Gemma 2B-it</h1>\n",
    "   <p style=\"font-size: 18px; margin-bottom: 2px; opacity: 0.65;\">Preg√∫ntame cualquier duda...</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "css = \"\"\"\n",
    "h1 {\n",
    "  text-align: center;\n",
    "  display: block;\n",
    "}\n",
    "#duplicate-button {\n",
    "  margin: auto;\n",
    "  color: white;\n",
    "  background: #1565c0;\n",
    "  border-radius: 100vh;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "803061b7-6a38-413d-801b-250133de96a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serchiboi/Desktop/python-ws/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662d842b6b1443b391c5cfcbebf20977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serchiboi/Desktop/python-ws/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Set the model\n",
    "model_id = \"google/gemma-1.1-2b-it\"\n",
    "\n",
    "#bits and bytes config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, #Load the model in 4 bits\n",
    "    bnb_4bit_quant_type=\"nf4\", #Quantization type 4 bits\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 #Data type for calculation\n",
    ")\n",
    "\n",
    "# Load the pre-trained tokenizer associated with the model specified by `model_id`\n",
    "# `padding_side=\"right\"` indicates that padding should be applied to the right side of the sequences\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"right\")\n",
    "\n",
    "# Load the pre-trained causal language model associated with `model_id`\n",
    "# `quantization_config=bnb_config` specifies the quantization configuration to load the model in 4 bits\n",
    "# `device_map={\"\":0}` assigns the model to the first available CUDA device, typically the first GPU (index 0)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "\n",
    "# Definition of terminators\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id, # ID of the end-of-sequence token\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\") # ID of the blank token (in this case, assumed to represent the end of the sequence)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08bb771a-b3a4-405f-8d97-d6c127870747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to translate from Spanish to English and vice versa\n",
    "def translate_text(text,mode):\n",
    "        try:\n",
    "            # print(\"Traduciendo:\",text,\"--\",type(text))\n",
    "            # Define the source and destination of the translation\n",
    "            src = 'es' if mode else 'en'\n",
    "            dest = 'en' if mode else 'es'\n",
    "            #print(src,\"--\",dest)\n",
    "\n",
    "            # Call the Google API to do the translation\n",
    "            translated_text = GoogleTranslator(source=src, target=dest).translate(text) \n",
    "            # print(\"Resultado:\",translated_text,type(translated_text))\n",
    "            \n",
    "            return translated_text\n",
    "        except Exception as e:\n",
    "            # print(f\"Error al traducir:{text} --->{e}\")\n",
    "            return \"Tuve un problema de traducci√≥n ¬øpuedes volver a preguntar?\"  # In case of error, returns the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "098a8f7a-2697-41c7-98cd-4d2626849812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gemma_2b(message: str, \n",
    "              history: list, \n",
    "              temperature: float, \n",
    "              max_new_tokens: int\n",
    "             ) -> str:\n",
    "    \"\"\"\n",
    "    Generate a streaming response.\n",
    "    Args:\n",
    "        message (str): The input message.\n",
    "        history (list): The conversation history used by ChatInterface.\n",
    "        temperature (float): The temperature for generating the response.\n",
    "        max_new_tokens (int): The maximum number of new tokens to generate.\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "    message = translate_text(message,True) # Translate es to en\n",
    "\n",
    "    # Concat all the conversation to use as a context\n",
    "    # The convesarion has format with specifict format\n",
    "    conversation = []\n",
    "    for user, assistant in history:\n",
    "        conversation.extend([\n",
    "                             {\"role\": \"user\", \"content\": user}, # Add the user message\n",
    "                             {\"role\": \"assistant\", \"content\": assistant}# Add the assistant response\n",
    "                            ])\n",
    "    conversation.append({\"role\": \"user\", \"content\": message}) # add user current message\n",
    "\n",
    "    # Convert the conversation history into input IDs using the tokenizer\n",
    "    input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Initialize the text streamer to handle the output text generation\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    generate_kwargs = dict(\n",
    "        input_ids= input_ids, # Input IDs generated from the conversation history\n",
    "        streamer=streamer, # Text streamer for handling the output text generation\n",
    "        max_new_tokens=max_new_tokens, # Maximum number of new tokens to generate\n",
    "        do_sample=True, # Enable sampling for generating the output\n",
    "        temperature=temperature, # Temperature setting to control randomness\n",
    "        eos_token_id=terminators, # End-of-sequence token IDs\n",
    "    )\n",
    "    \n",
    "    # This will enforce greedy generation (do_sample=False) when the temperature is passed 0, avoiding the crash.             \n",
    "    if temperature == 0:\n",
    "        generate_kwargs['do_sample'] = False\n",
    "\n",
    "    # Start a new thread to run the model's generate method with the specified arguments\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    #outputs = []\n",
    "    #for text in streamer:\n",
    "        #outputs.append(text)\n",
    "        #print(outputs)\n",
    "        #yield \"\".join(outputs)\n",
    "    partial_message = \"\"\n",
    "    for new_token in streamer:\n",
    "        partial_message += new_token\n",
    "    partial_message = translate_text(partial_message,False)\n",
    "    yield partial_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e1128676-c60f-4acf-bbdd-352bc8de3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to be called when the button is clicked\n",
    "momento_inicial = None\n",
    "def on_button_click():\n",
    "    global momento_inicial\n",
    "    if momento_inicial is None:\n",
    "        momento_inicial = datetime.now()\n",
    "    else:\n",
    "        diferencia = datetime.now() - momento_inicial\n",
    "        intervalo_deseado = timedelta(minutes=5)\n",
    "        if diferencia >= intervalo_deseado:\n",
    "            momento_inicial = None\n",
    "            return on_button_click()\n",
    "        else:\n",
    "            tiempo_restante = intervalo_deseado - (datetime.now() - momento_inicial)\n",
    "            tiempo_restante_str = str(tiempo_restante).split('.')[0]\n",
    "            return f\"Ya se tiene en cola un reporte hace menos de 5 minutos. Podr√°s enviar uno en {tiempo_restante_str}.\"\n",
    "    return \"¬°Aviso enviado, gracias por el reporte!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "14f6060c-a77e-45c4-a4ef-c4a612f65bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio block\n",
    "chatbot=gr.Chatbot(height=450, placeholder=PLACEHOLDER, label='Gradio ChatInterface')\n",
    "\n",
    "with gr.Blocks(fill_height=True, css=css) as demo:\n",
    "    \"\"\"\n",
    "    Create a Gradio interface for the GEMMA 2B chat model.\n",
    "\n",
    "    Parameters:\n",
    "    fill_height (bool): Specifies if the height of the interface should be filled.\n",
    "    css (str): Custom CSS for styling the interface.\n",
    "    \"\"\"\n",
    "    \n",
    "    gr.Markdown(DESCRIPTION) # Add a Markdown section with the description of the interface\n",
    "\n",
    "    # Add a custom button and connect it to the on_button_click function\n",
    "    custom_button = gr.Button(\"Aviso de p√°nico\")\n",
    "    output_text = gr.Textbox(label=\"Mensaje aviso de p√°nico\")\n",
    "\n",
    "    custom_button.click(on_button_click, inputs=[], outputs=output_text)\n",
    "    \n",
    "    # Create a chat interface for interacting with the GEMMA 2B model\n",
    "    gr.ChatInterface(\n",
    "        fn=chat_gemma_2b, # Function to call for generating responses\n",
    "        chatbot=chatbot,  # Chatbot object to handle the conversation\n",
    "        fill_height=True, # Specifies if the height of the chat interface should be filled\n",
    "        additional_inputs_accordion=gr.Accordion(label=\"‚öôÔ∏è Parameters\", open=False, render=False), # Accordion for additional input parameters\n",
    "        additional_inputs=[\n",
    "            # Slider for adjusting the temperature parameter\n",
    "            gr.Slider(minimum=0,\n",
    "                      maximum=1, \n",
    "                      step=0.1,\n",
    "                      value=0.95, \n",
    "                      label=\"Temperature\", \n",
    "                      render=False),\n",
    "            # Slider for adjusting the maximum number of new tokens\n",
    "            gr.Slider(minimum=128, \n",
    "                      maximum=4096,\n",
    "                      step=1,\n",
    "                      value=512, \n",
    "                      label=\"Max new tokens\", \n",
    "                      render=False ),\n",
    "            ],\n",
    "        examples=[\n",
    "            ['¬øC√≥mo establecer una base humana en Marte? Da una respuesta breve.'],\n",
    "            ['Expl√≠came la teor√≠a de la relatividad como si tuviera 8 a√±os.'],\n",
    "            ['¬øCu√°nto es 9,000 * 9,000?'],\n",
    "            ['Escribe un mensaje de cumplea√±os lleno de juegos de palabras para mi amigo Alex.'],\n",
    "            ['Justifica por qu√© un ping√ºino podr√≠a ser un buen rey de la jungla.']\n",
    "            ],\n",
    "        cache_examples=False, # Do not cache the examples\n",
    "                     )\n",
    "\n",
    "\n",
    "    # Add a Markdown section with the license information\n",
    "    gr.Markdown(LICENSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bd7da0b6-66a0-4bda-959e-563b59b03f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7889\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7889/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d5fc481f-ad33-4041-9e83-e5e027946ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n",
      "Running on public URL: https://341926d3912da37424.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://341926d3912da37424.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a7ce1-6c09-47f3-aea4-90f64c4d155b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
