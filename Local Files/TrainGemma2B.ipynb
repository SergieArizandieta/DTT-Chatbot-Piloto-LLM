{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6afd2eeb-1867-4510-beba-04fb5646ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08f7596-66a9-465c-88fe-bf38cf810eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Free GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bfbcacd-2684-4171-b465-56fdcb40e3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serchiboi/Desktop/python-ws/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c449ee1ebf4e474ca0778616e849c87f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serchiboi/Desktop/python-ws/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Set the model\n",
    "model_id = \"google/gemma-1.1-2b-it\"\n",
    "\n",
    "#bits and bytes config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, #Load the model in 4 bits\n",
    "    bnb_4bit_quant_type=\"nf4\", #Quantization type 4 bits\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 #Data type for calculation\n",
    ")\n",
    "\n",
    "# Load the pre-trained tokenizer associated with the model specified by `model_id`\n",
    "# `padding_side=\"right\"` indicates that padding should be applied to the right side of the sequences\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"right\")\n",
    "\n",
    "# Load the pre-trained causal language model associated with `model_id`\n",
    "# `quantization_config=bnb_config` specifies the quantization configuration to load the model in 4 bits\n",
    "# `device_map={\"\":0}` assigns the model to the first available CUDA device, typically the first GPU (index 0)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b54c641-165d-4a14-abd9-77e33b845678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an environment variable to enable Weights and Biases (wandb) logging\n",
    "# Weights and Biases (wandb) is a tool for tracking machine learning experiments\n",
    "# Setting \"WANDB_DISABLED\" to \"false\" enables wandb logging\n",
    "os.environ[\"WANDB_DISABLED\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "138669d7-bcc0-428a-9998-d8385f55bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA (Low-Rank Adaptation) Configuration for Fine-Tuning a Pretrained Model\n",
    "lora_config = LoraConfig(\n",
    "    r=64, # Rank of the low-rank decomposition. Determines the capacity of the LoRA model.\n",
    "    lora_alpha=32, # Scales the values of the adapted parameters. A higher value implies a greater contribution of adaptation.\n",
    "    lora_dropout=0.05, # Dropout probability for the adapted layers. Helps prevent overfitting.\n",
    "    bias=\"none\", # Bias configuration. \"none\" indicates no additional bias is added in the adapted layers.\n",
    "    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"], # List of module names where LoRA adaptation is applied.\n",
    "    task_type = \"CAUSAL_LM\", # Type of task for which the model is configured. \"CAUSAL_LM\" refers to a causal language model.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12c94e5f-e61a-4e49-9858-fe59d77401e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Question', 'Answer'],\n",
       "        num_rows: 496\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = load_dataset(\"SerchiBoi/DTT-Info\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf6a58fe-a629-497d-985a-6e0c7e3e531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to set a format of the training data\n",
    "# This has a specific format to works with the previusa dataset\n",
    "def formatting_func(example):\n",
    "    text = f\"Question: {example['Question'][0]}\\Answer: {example['Answer'][0]}\"\n",
    "    return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10ca7636-e664-4ed1-a560-b358d71b6504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model, # Model to be used for training\n",
    "    train_dataset=data[\"train\"], # Training dataset\n",
    "    peft_config=lora_config,  # Specific configuration of the generation algorithm\n",
    "    formatting_func=formatting_func, # Function to format dataset instances\n",
    "    max_seq_length=1300, # Maximum allowed input sequence length\n",
    "    args=transformers.TrainingArguments( # Training arguments\n",
    "        per_device_train_batch_size=1, # Training batch size per device\n",
    "        gradient_accumulation_steps=4, # Number of gradient accumulation steps\n",
    "        warmup_steps=2, # Number of warmup steps for the optimizer\n",
    "        max_steps=1, # Maximum number of training steps\n",
    "        learning_rate=2e-4, # Learning rate for the optimizer\n",
    "        fp16=True, # Enable 16-bit precision for training\n",
    "        logging_steps=1, # Steps interval for logging\n",
    "        output_dir=\"outputs-dtt-v1\", # Output directory to save training results\n",
    "        optim=\"paged_adamw_8bit\", # Name of the optimizer used for training\n",
    "        report_to=\"tensorboard\", # Destination to report training execution\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "737c3c1d-ed8c-492c-9d67-12e3a6924d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.301500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=1.3015427589416504, metrics={'train_runtime': 0.6366, 'train_samples_per_second': 6.283, 'train_steps_per_second': 1.571, 'total_flos': 2472397209600.0, 'train_loss': 1.3015427589416504, 'epoch': 1.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "553a8d93-1e64-45f9-ad64-e7e6e016f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 48300), started 0:00:07 ago. (Use '!kill 48300' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bdd640fb06671ad1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bdd640fb06671ad1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the extension of TensorBoard\n",
    "%load_ext tensorboard \n",
    "#Start TensorBoard using the info of the directory specified in logdir\n",
    "%tensorboard --logdir=outputs-dtt-v1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022488d-9839-458c-9445-ae5643a00e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
