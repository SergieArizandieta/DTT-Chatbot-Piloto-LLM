{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a60ddbf-5d59-4d93-bee4-f1771fa8a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7986f0bc-0872-48e1-a7ae-851b76666041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unsloth/gemma-2b-it-bnb-4bit\n",
    "#SerchiBoi/DTT-Chatbot-Piloto\n",
    "#SerchiBoi/DTT-Chatbot-Piloto-v1\n",
    "\n",
    "#DTT-unsloth-gemma-2b-it-v1\n",
    "#DTT-unsloth-gemma-2b-it-v2\n",
    "#DTT-unsloth-gemma-2b-it-v3\n",
    "#DTT-unsloth-gemma-2b-it-v4\n",
    "#DTT-unsloth-gemma-2b-it-v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "232f319b-d867-4c97-b099-8f37626b53eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d0869a-f526-49f2-972f-30a956b36d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"SerchiBoi/DTT-Chatbot-Piloto-v1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76cf4d8-a6d2-4d1a-a11d-7126ce78eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a53bb0b1-3fd0-486d-b906-c244bded0d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f4c78a423d4d15a440512c4c9b748a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Gemma patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3050. Max memory: 7.779 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606ee0f0ae224bc494eadca1ad1925e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e030895e153444897e072b1dad6d74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cac4fb48fe4b8287af8267f572d7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 18 layers with 18 QKV layers, 18 O layers and 18 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model, # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4fe38fc-3b5b-4dc2-a28b-3395b0fabf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Gmail Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7610bb4-c145-45b3-b183-10ebd033da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to be called when the button is clicked\n",
    "from datetime import datetime, timedelta\n",
    "momento_inicial = None\n",
    "def on_button_click():\n",
    "    global momento_inicial\n",
    "    if momento_inicial is None:\n",
    "        momento_inicial = datetime.now()\n",
    "    else:\n",
    "        diferencia = datetime.now() - momento_inicial\n",
    "        intervalo_deseado = timedelta(minutes=5)\n",
    "        if diferencia >= intervalo_deseado:\n",
    "            momento_inicial = None\n",
    "            return on_button_click()\n",
    "        else:\n",
    "            tiempo_restante = intervalo_deseado - (datetime.now() - momento_inicial)\n",
    "            tiempo_restante_str = str(tiempo_restante).split('.')[0]\n",
    "            return f\"Ya se tiene en cola un reporte hace menos de 5 minutos. Podr√°s enviar uno en {tiempo_restante_str}.\"\n",
    "    sendMils()\n",
    "    return \"¬°Aviso enviado, gracias por el reporte!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a232e6bb-f3d8-4e6e-adbb-6813ff609c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "def sendMils():\n",
    "    me =  \"dttchatbot@gmail.com\"\n",
    "    family = [\n",
    "        \"sergieari@gmail.com\",\n",
    "        \"3000523830101@ingenieria.usac.edu.gt\",\n",
    "        \"fedeloboggonzales@gmail.com\",\n",
    "        me\n",
    "    ]\n",
    "    \n",
    "    subject = \"Aviso de panico\"\n",
    "    message = \"Se ha recibido una alerta de panico.\"\n",
    "    \n",
    "    text = f\"Subject: {subject}\\n\\n{message}\"\n",
    "    \n",
    "    server = smtplib.SMTP(\"smtp.gmail.com\",587)\n",
    "    server.starttls()\n",
    "    \n",
    "    server.login(me,\"pldhxmaifoajsiyr\")\n",
    "    \n",
    "    for reciber in family:\n",
    "        server.sendmail(me,reciber,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3852f896-c209-48cc-b9da-442ee343f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Traslate Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a135fe1d-6c60-4acc-9426-35e92f8b2775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to translate from Spanish to English and vice versa\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def translate_text(text,mode):\n",
    "        try:\n",
    "            # print(\"Traduciendo:\",text,\"--\",type(text))\n",
    "            # Define the source and destination of the translation\n",
    "            src = 'es' if mode else 'en'\n",
    "            dest = 'en' if mode else 'es'\n",
    "            #print(src,\"--\",dest)\n",
    "\n",
    "            # Call the Google API to do the translation\n",
    "            translated_text = GoogleTranslator(source=src, target=dest).translate(text) \n",
    "            # print(\"Resultado:\",translated_text,type(translated_text))\n",
    "            \n",
    "            return translated_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error al traducir:{text} --->{e}\")\n",
    "            return \"Tuve un problema de traducci√≥n ¬øpuedes volver a preguntar?\"  # In case of error, returns the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30da0a8c-b0ad-419b-b81f-cd0b3714d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Chat Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3db0fe3e-6348-4535-8ba9-d1f5c25c7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import TextStreamer,TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "def chat_gemma_2b(message: str, history: list  ) -> str:\n",
    "    message = translate_text(message, True)  # Translate es to en\n",
    "\n",
    "    #for user, assistant in history:\n",
    "    #    conversation.extend([\n",
    "    #                         {\"role\": \"user\", \"content\": translate_text(user,True)}, # Add the user message\n",
    "    #                         #{\"role\": \"user\", \"content\": user}, # Add the user message\n",
    "    #                         {\"role\": \"assistant\", \"content\": translate_text(assistant,True)}# Add the assistant response\n",
    "    #                         #{\"role\": \"assistant\", \"content\": assistant}# Add the assistant response\n",
    "    #                        ])\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize = True, \n",
    "        add_generation_prompt = True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    text_streamer = TextIteratorStreamer(\n",
    "        tokenizer, \n",
    "        timeout=10.0,\n",
    "        skip_prompt=True, \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    generate_kwargs = dict(\n",
    "        input_ids= input_ids,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=2048,\n",
    "        use_cache=True\n",
    "        #do_sample=True,\n",
    "    )\n",
    "\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    response = \"\"\n",
    "    for new_text in text_streamer:\n",
    "        response += new_text\n",
    "        yield response\n",
    "    response = translate_text(response,False)\n",
    "    response = response.replace(\"TDT\", \"DTT\")\n",
    "    yield response\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee6f599c-9a77-401f-97d2-50352d9da722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Gradio Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21fbb24a-3998-40cc-98fc-16fef7431073",
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCRIPTION = '''\n",
    "<div>\n",
    "<h1 style=\"text-align: center;\">DTT Chatbot Piloto</h1>\n",
    "<h3></h3>\n",
    "<h2>ü¶ï Este chatbot piloto est√° dise√±ado para responder preguntas relacionadas con las pr√°cticas de la ECYS, centr√°ndose especialmente en las pr√°cticas finales vinculadas al proyecto DTT.</h2>\n",
    "<h3></h3>\n",
    "<h2>‚úÖ Recuerda completar el <a href=\"https://forms.gle/7a6A3mx3fTj8Xo3aA\"><b>fomulario con tu experiencia</b></a>, para su posterior an√°lisis.</h2>\n",
    "<h3></h3>\n",
    "<h3>‚ö†Ô∏è Para obtener respuestas precisas del chatbot, es esencial escribir preguntas correctamente y usar signos de puntuaci√≥n adecuadamente. Esto ayuda a la comprensi√≥n y a obtener respuestas r√°pidas y precisas. </h3>\n",
    "<h3></h3>\n",
    "<h3></h3>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "PANIC = '''\n",
    "<div>\n",
    "<h1>‚Ää</h1>\n",
    "<h1>‚ùó¬øEl chatbot est√° tardando mucho en responder o no responde?‚ùó </h1>\n",
    "<h3>¬°Presiona el bot√≥n de \"<b>Aviso de p√°nico</b>\" para enviar un aviso y que el chatbot sea reseteado manualmente!</h3>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "FOOTER = '''\n",
    "<div>\n",
    "<h1>‚Ää</h1>\n",
    "<h1>üöè‚ÄäEspecificaciones del chatbot üöè‚Ää</h1>\n",
    "<h2>üí° Aqu√≠ puedes encontrar el <a href=\"https://huggingface.co/datasets/SerchiBoi/DTT-Info\"><b>conjunto de datos de entrenamiento</b></a></h2>\n",
    "<h2>üì¢ Aqu√≠ puedes encontrar el <a href=\"https://huggingface.co/SerchiBoi/DTT-Chatbot-Piloto-v1\"><b>modelo actualmente usado</b></a></h2>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "LICENSE = \"\"\"\n",
    "<p/>\n",
    "---\n",
    "Built with Gemma 2B-it\n",
    "\"\"\"\n",
    "\n",
    "PLACEHOLDER = \"\"\"\n",
    "<div style=\"padding: 30px; text-align: center; display: flex; flex-direction: column; align-items: center;\">\n",
    "   <img src=\"https://www.lavanguardia.com/andro4all/hero/2024/04/google-gemma.png?width=1200&aspect_ratio=16:9\" style=\"width: 80%; max-width: 550px; height: auto; opacity: 0.55;\"> \n",
    "   <h1 style=\"font-size: 28px; margin-bottom: 2px; opacity: 0.55;\">Gemma 2B-it</h1>\n",
    "   <p style=\"font-size: 18px; margin-bottom: 2px; opacity: 0.65;\">Preg√∫ntame cualquier duda...</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "css = \"\"\"\n",
    "h1 {\n",
    "  text-align: center;\n",
    "  display: block;\n",
    "}\n",
    "#duplicate-button {\n",
    "  margin: auto;\n",
    "  color: white;\n",
    "  background: #1565c0;\n",
    "  border-radius: 100vh;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b006efe-6822-445b-92ce-c0da3ff1df57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.31.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "IMPORTANT: You are using gradio version 4.31.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Gradio block\n",
    "chatbot=gr.Chatbot(height=450, placeholder=PLACEHOLDER, label='Gradio ChatInterface')\n",
    "\n",
    "with gr.Blocks(fill_height=True, css=css) as demo:\n",
    "    \"\"\"\n",
    "    Create a Gradio interface for the GEMMA 2B chat model.\n",
    "\n",
    "    Parameters:\n",
    "    fill_height (bool): Specifies if the height of the interface should be filled.\n",
    "    css (str): Custom CSS for styling the interface.\n",
    "    \"\"\"\n",
    "    gr.Markdown(DESCRIPTION) # Add a Markdown section with the description of the interface\n",
    "    \n",
    "    # Create a chat interface for interacting with the GEMMA 2B model\n",
    "    gr.ChatInterface(\n",
    "        fn=chat_gemma_2b, # Function to call for generating responses\n",
    "        chatbot=chatbot,  # Chatbot object to handle the conversation\n",
    "        fill_height=True, # Specifies if the height of the chat interface should be filled\n",
    "        additional_inputs_accordion=gr.Accordion(label=\"‚öôÔ∏è Parameters\", open=False, render=False), # Accordion for additional input parameters\n",
    "        examples=[\n",
    "\n",
    "            ['¬øQu√© es la Universidad de San Carlos de Guatemala?'],\n",
    "            ['¬øQu√© es la Escuela de Ciencias y Sistemas de la Universidad de San Carlos de Guatemala?'],\n",
    "            ['¬øCu√°l es el objetivo principal del proyecto ‚ÄúDesarrollo de Transferencia Tecnol√≥gica (DTT) en la Escuela de Ingenier√≠a en Ciencias y Sistemas de la Facultad de Ingenier√≠a, USAC?'],\n",
    "            ['¬øC√≥mo se dividen las pr√°cticas de ingenier√≠a?|'],\n",
    "            ['¬øCu√°nto tiempo dura la pr√°ctica inicial?'],\n",
    "            ['¬øCu√°nto tiempo dura la pr√°ctica intermedia?'],\n",
    "            ['¬øCu√°nto tiempo dura la pr√°ctica final laboral?'],\n",
    "            ['¬øCu√°nto tiempo dura la pr√°ctica de empresarios juveniles?'],\n",
    "            ['¬øQu√© se considera una falta leve durante las Pr√°cticas Finales?'],\n",
    "            ['¬øQu√© se considera una falta grave durante las Pr√°cticas Finales?'],\n",
    "            ['¬øCu√°les son las competencias que debe tener un egresado de la Licenciatura en Ingenier√≠a en Ciencias y Sistemas?'],\n",
    "            ['¬øQu√© correo se debe utilizar en el proceso de pr√°cticas docentes dentro del proyecto DTT?'],\n",
    "            ['¬øComo creo la ponderaci√≥n de un curso en DTT?'],\n",
    "            ['¬øCu√°l es el prop√≥sito de los laboratorios de la Escuela de Ciencias y Sistemas?'],\n",
    "            ['¬øD√≥nde puedo encontrar informaci√≥n sobre el Acuerdo de Cumplimiento √âtico y Profesional de la Pr√°ctica Docente?'],\n",
    "            \n",
    "\n",
    "            ['¬øDonde puedo encontrar el documento Manual de comportamiento?'],\n",
    "            ['¬øDonde puedo encontrar la plantilla de actividades?'],\n",
    "            ['¬øDonde puedo encontrar la plantilla de inicio?'],\n",
    "            ['¬øDonde puedo encontrar el documento Manual DSI-104?'],\n",
    "            ['¬øDonde puedo encontrar la plantilla de foros?'],\n",
    "            ['¬øDonde puedo el link de entrega de foros?'],\n",
    "            ['¬øDonde puedo el link del documento de control de asistencia?'],\n",
    "            ['¬øDonde puedo el link de entrega de grabaciones?'],\n",
    "            ['¬øDonde puedo el link de entrega de hojas de calificaci√≥n?'],\n",
    "            ['¬øDonde puedo encontrar el documento Perfil de Egreso del Estudiante de Ingenier√≠a en Ciencias y Sistemas?'],\n",
    "            ['¬øDonde puedo encontrar el documento Procedimiento de Conferencias Online?'],\n",
    "            ['¬øDonde puedo encontrar el documento Proyecto DTT?'],\n",
    "            ['¬øDonde puedo encontrar el ejemplo de como crear la ponderaci√≥n en DTT?'],\n",
    "            ['¬øDonde puedo encontrar el ejemplo del formato del reporte de conferencia?'],\n",
    "            ['¬øDonde puedo encontrar el ejemplo del reporte mensual DTT?'],\n",
    "            ['¬øDonde puedo encontrar el documento de Acuerdo Anti-Plagio?'],\n",
    "            ['¬øDonde puedo encontrar la plantilla Formato Reporte de conferencia?'],\n",
    "            ['¬øDonde puedo encontrar la plantilla de SLA de Cumplimiento Etico y Profesional de Practica Proyecto DTT?'],\n",
    "            ['¬øDonde puedo encontrar la plantilla de Hoja de Calificaci√≥n?'],\n",
    "            ['¬øDonde puedo encontrar gu√≠a para la elaboraci√≥n de un art√≠culo?']\n",
    "            ],\n",
    "        cache_examples=False, # Do not cache the examples\n",
    "        examples_per_page=15\n",
    "    )\n",
    "    gr.Markdown(PANIC) # Add a Markdown section with the description of the interface\n",
    "    # Add a custom button and connect it to the on_button_click function\n",
    "    custom_button = gr.Button(\"Aviso de p√°nico\")\n",
    "    output_text = gr.Textbox(label=\"Resultado del aviso:\")\n",
    "    custom_button.click(on_button_click, inputs=[], outputs=output_text)\n",
    "\n",
    "    gr.Markdown(FOOTER)\n",
    "\n",
    "    # Add a Markdown section with the license information\n",
    "    gr.Markdown(LICENSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "470279e0-26a1-460d-9ec9-40921d20421b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7da7dd5e-af0f-4e78-9087-392ddabf92e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://cc427a431bfa5e2c2e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cc427a431bfa5e2c2e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(share=True)\n",
    "#demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb0b460-5244-4de7-ae12-43dda2a9c955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb723306-0852-45c3-9b89-222e1e377108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
